{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT and LoRA Experiment Runner\n",
    "\n",
    "This notebook runs the full pipeline for preprocessing data and training the three different models:\n",
    "1.  **Small-BERT (From Scratch):** A 4-layer BERT trained from random initialization.\n",
    "2.  **TinyBERT (Full Fine-Tune):** A pre-trained TinyBERT with all parameters fine-tuned.\n",
    "3.  **TinyBERT (LoRA Fine-Tune):** A pre-trained TinyBERT fine-tuned using our custom LoRA implementation.\n",
    "\n",
    "Finally, it visualizes the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Install the required packages and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!mkdir -p data/raw models logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Required:** Before proceeding, you must download the dataset (`complaints_small.csv`) and place it in the `data/raw/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part 1: Data Preprocessing\n",
    "\n",
    "This step loads the raw data, cleans it, and creates the `train.csv`, `test.csv`, and `label_map.json` files in `data/processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/preprocess.py --input-file data/raw/complaints_small.csv --output-dir data/processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part 2: Train Small-BERT (From Scratch)\n",
    "\n",
    "This trains the 4-layer BERT model. Results will be saved in `models/bert_scratch/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train_bert_scratch.py --data-dir data/processed --output-dir models/bert_scratch --num-epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part 3: Train TinyBERT (Full Fine-Tune)\n",
    "\n",
    "This trains the full fine-tuning baseline. Results will be saved in `models/tinybert_full/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train_full_finetune.py --data-dir data/processed --output-dir models/tinybert_full --num-epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Part 4: Train TinyBERT (LoRA Fine-Tune)\n",
    "\n",
    "This trains the LoRA model using our custom implementation. Results will be saved in `models/tinybert_lora/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/train_lora.py --data-dir data/processed --output-dir models/tinybert_lora --num-epochs 3 --lora-r 8 --lora-alpha 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis and Visualization\n",
    "\n",
    "Let's load the `results.json` file from each experiment and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "result_files = [\n",
    "    \"models/bert_scratch/results.json\",\n",
    "    \"models/tinybert_full/results.json\",\n",
    "    \"models/tinybert_lora/results.json\"\n",
    "]\n",
    "\n",
    "data = []\n",
    "for f in result_files:\n",
    "    try:\n",
    "        with open(f, 'r') as file:\n",
    "            data.append(json.load(file))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {f} not found. Did the script run correctly?\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index(\"model\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot 1: F1-Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=df.index, y=\"test_f1_score\", data=df)\n",
    "plt.title(\"Model Comparison: Weighted F1-Score\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylim(0.8, df['test_f1_score'].max() * 1.05) # Adjust ylim for better visibility\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Trainable Parameters (Log Scale)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=df.index, y=\"trainable_params\", data=df)\n",
    "ax.set_yscale(\"log\")\n",
    "plt.title(\"Model Comparison: Trainable Parameters (Log Scale)\")\n",
    "plt.ylabel(\"Trainable Parameters (Log)\")\n",
    "plt.xlabel(\"Model\")\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():,.0f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 9), textcoords='offset points')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
